# Classical-ML-papers
An ever-growing collection of classical and must-read papers in Machine Learning

## Causal Machine Learning:
Github repository "Must-read recent papers and resources on {Causal}‚à©{ML}": https://github.com/jvpoulos/causal-ml
## Convolutional Neural Network
1. Krizhevsky, Alex & Sutskever, Ilya & Hinton, Geoffrey. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Neural Information Processing Systems. 25. 10.1145/3065386.  https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
2. Fukushima, K. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biol. Cybernetics 36, 193‚Äì202 (1980). https://doi.org/10.1007/BF00344251. https://link.springer.com/article/10.1007/BF00344251
3. Y. Lecun, L. Bottou, Y. Bengio and P. Haffner, "Gradient-based learning applied to document recognition," in Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, Nov. 1998, doi: 10.1109/5.726791. http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf
4. E. Shelhamer, J. Long and T. Darrell, "Fully Convolutional Networks for Semantic Segmentation," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 4, pp. 640-651, 1 April 2017, doi: 10.1109/TPAMI.2016.2572683. https://arxiv.org/abs/1411.4038
5. Waibel, Alexander & Hanazawa, Toshiyuki & Hinton, G. & Shikano, Kiyohiro & Lang, K.J.. (1989). Phoneme recognition using time-delay neural networks. Acoustics, Speech and Signal Processing, IEEE Transactions on. 37. 328 - 339. 10.1109/29.21701. https://www.cs.toronto.edu/~fritz/absps/waibelTDNN.pdf
<!-- 

#### Inception
1. Going Deeper with Convolutions (2014), Szegedy et al., @ai.google + @Github üíΩ.
2. Rethinking the Inception Architecture for Computer Vision (2016), Szegedy, Vanhoucke, Ioffe, Shlens, and Wojna, @ai.google üî¨.
3. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning (2016), Szegedy, Ioffe, Vanhoucke, and Alemi, @ai.google üî¨.

#### U-Net (image segmentation CNN)

1. U-Net: Convolutional Networks for Biomedical Image Segmentation (2015), Ronneberger, Fischer, Brox, @Springer üîí / @arXiv üîë.

#### VGG (image recognition CNN)

1. Very Deep Convolutional Networks for Large-Scale Image Recognition (2015), Simonyan and Zisserman, @arXiv + @author üåê + @ICLR üìä + @YouTube üé•.
-->

## üìä Data pre-processing
1. Wickham, Hadley. (2014). Tidy data. The American Statistician. 14. 10.18637/jss.v059.i10. https://vita.had.co.nz/papers/tidy-data.pdf
2. Singh, Dalwinder & Singh, Birmohan. (2019). Investigating the impact of data normalization on classification performance. Applied Soft Computing. 105524. 10.1016/j.asoc.2019.105524. 

## Datasets
Most complete list ever found: https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research

## Decision trees
1. Quinlan, J.R. Induction of decision trees. Mach Learn 1, 81‚Äì106 (1986). https://doi.org/10.1007/BF00116251. https://link.springer.com/article/10.1007/BF00116251



## Face Recognition
1. Mei, Wang & Deng, Weihong. (2018). Deep Face Recognition: A Survey. Neurocomputing. 429. 10.1016/j.neucom.2020.10.081. https://arxiv.org/pdf/1804.06655.pdf
2. Zhao, Wen et al. ‚ÄúFace recognition: A literature survey.‚Äù ACM Comput. Surv. 35 (2003): 399-458. https://inc.ucsd.edu/mplab/users/marni/Igert/Zhao_2003.pdf
3. Y. Taigman, M. Yang, M. Ranzato and L. Wolf, "DeepFace: Closing the Gap to Human-Level Performance in Face Verification," 2014 IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 1701-1708, doi: 10.1109/CVPR.2014.220. https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf

## Generative Adversarial Network

1. Goodfellow, Ian & Pouget-Abadie, Jean & Mirza, Mehdi & Xu, Bing & Warde-Farley, David & Ozair, Sherjil & Courville, Aaron & Bengio, Y.. (2014). Generative Adversarial Networks. Advances in Neural Information Processing Systems. 3. 10.1145/3422622. https://arxiv.org/abs/1406.2661


## Long Short-Term Memory (LSTM)
1. Sepp Hochreiter, J√ºrgen Schmidhuber; Long Short-Term Memory. Neural Comput 1997; 9 (8): 1735‚Äì1780. doi: https://doi.org/10.1162/neco.1997.9.8.1735. http://www.bioinf.jku.at/publications/older/2604.pdf
2. Sherstinsky, Alex. ‚ÄúFundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network.‚Äù ArXiv abs/1808.03314 (2018): n. pag. https://arxiv.org/abs/1808.03314?t

## Residual Neural Network (ResNet)

1. He, Kaiming & Zhang, Xiangyu & Ren, Shaoqing & Sun, Jian. (2016). Deep Residual Learning for Image Recognition. 770-778. 10.1109/CVPR.2016.90. https://arxiv.org/abs/1512.03385

## Transformer (sequence to sequence modeling)

1. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17). Curran Associates Inc., Red Hook, NY, USA, 6000‚Äì6010. https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf

  




## Ensemble Methods
#### AdaBoost

1. Freund, Y., Schapire, R.E. (1995). A desicion-theoretic generalization of on-line learning and an application to boosting. In: Vit√°nyi, P. (eds) Computational Learning Theory. EuroCOLT 1995. Lecture Notes in Computer Science, vol 904. Springer, Berlin, Heidelberg. https://doi.org/10.1007/3-540-59119-2_166. https://www.sciencedirect.com/science/article/pii/S002200009791504X
2. Freund, Yoav and Robert E. Schapire. ‚ÄúExperiments with a New Boosting Algorithm.‚Äù ICML (1996). https://cseweb.ucsd.edu/~yfreund/papers/boostingexperiments.pdf

#### Bagging

1. Breiman, L. Bagging predictors. Mach Learn 24, 123‚Äì140 (1996). https://doi.org/10.1007/BF00058655. https://www.stat.berkeley.edu/~breiman/bagging.pdf

#### Gradient Boosting

1. Jerome H. Friedman "Greedy function approximation: A gradient boosting machine.," The Annals of Statistics, Ann. Statist. 29(5), 1189-1232, (October 2001) . https://jerryfriedman.su.domains/ftp/trebst.pdf
2. Chen, Tianqi and Carlos Guestrin. ‚ÄúXGBoost: A Scalable Tree Boosting System.‚Äù Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2016):  https://arxiv.org/abs/1603.02754

#### Random Forest

1. Breiman, L. Random Forests. Machine Learning 45, 5‚Äì32 (2001). https://doi.org/10.1023/A:1010933404324. https://link.springer.com/article/10.1023/A:1010933404324

## Games
1. Silver, David & Huang, Aja & Maddison, Christopher & Guez, Arthur & Sifre, Laurent & Driessche, George & Schrittwieser, Julian & Antonoglou, Ioannis & Panneershelvam, Veda & Lanctot, Marc & Dieleman, Sander & Grewe, Dominik & Nham, John & Kalchbrenner, Nal & Sutskever, Ilya & Lillicrap, Timothy & Leach, Madeleine & Kavukcuoglu, Koray & Graepel, Thore & Hassabis, Demis. (2016). Mastering the game of Go with deep neural networks and tree search. Nature. 529. 484-489. 10.1038/nature16961. https://www.nature.com/articles/nature16961
2. Campbell, Murray et al. ‚ÄúDeep Blue.‚Äù Artif. Intell. 134 (2002): 57-83. https://www.sciencedirect.com/science/article/pii/S0004370201001291



## Interpretability
Github repository "Interpretable Machine learning" https://github.com/grazianomita/machine_learning_interpretability_papers

## Optimization
#### Adam

1. Kingma, Diederik P. and Jimmy Ba. ‚ÄúAdam: A Method for Stochastic Optimization.‚Äù CoRR abs/1412.6980 (2015). https://arxiv.org/abs/1412.6980v8?hl=ja
2. Bergstra, James & Bengio, Y.. (2012). Random Search for Hyper-Parameter Optimization. The Journal of Machine Learning Research. 13. 281-305. https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf


#### Expectation Maximization

1. Maximum likelihood from incomplete data via the EM algorithm (1977), Dempster, Laird, and Rubin. https://www.ece.iastate.edu/~namrata/EE527_Spring08/Dempster77.pdf

#### Stochastic Gradient Descent

1. Kiefer, J. and Jacob Wolfowitz. ‚ÄúStochastic Estimation of the Maximum of a Regression Function.‚Äù Annals of Mathematical Statistics 23 (1952): 462-466. https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-23/issue-3/Stochastic-Estimation-of-the-Maximum-of-a-Regression-Function/10.1214/aoms/1177729392.full
2. Robbins, Herbert E.. ‚ÄúA Stochastic Approximation Method.‚Äù Annals of Mathematical Statistics 22 (2007): 400-407. https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full




<!-- 
## Miscellanea
#### Non-negative Matrix Factorization
1. Learning the parts of objects by non-negative matrix factorization (1999), Lee and Seung, @Nature üîí.
#### PageRank
1. The PageRank Citation Ranking: Bringing Order to the Web (1998), Page, Brin, Motwani, and Winograd, @CiteSeerX.
#### DeepQA (Watson)
1. Building Watson: An Overview of the DeepQA Project (2010), Ferrucci et al., @AAAI.
-->

## Natural Language Processing
1. Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. (2018). Improving language understanding by generative pre-training. https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf

2. Blei, David & Ng, Andrew & Jordan, Michael. (2001). Latent Dirichlet Allocation. The Journal of Machine Learning Research. 3. 601-608.  https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf

3. Deerwester, Scott C. et al. ‚ÄúIndexing by Latent Semantic Analysis.‚Äù J. Am. Soc. Inf. Sci. 41 (1990): 391-407. http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf

4. Mikolov, Tomas & Chen, Kai & Corrado, G.s & Dean, Jeffrey. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of Workshop at ICLR. 2013. https://arxiv.org/abs/1301.3781

## Neural Network 
#### Autograd
1. Paszke, Adam et al. ‚ÄúAutomatic differentiation in PyTorch.‚Äù (2017). https://openreview.net/pdf?id=BJJsrmfCZ


#### Back-propagation
1. Rumelhart, D., Hinton, G. & Williams, R. Learning representations by back-propagating errors. Nature 323, 533‚Äì536 (1986). https://doi.org/10.1038/323533a0. https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf

#### Batch Normalization
1. Ioffe, Sergey & Szegedy, Christian. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. https://arxiv.org/abs/1502.03167
2. Keskar, Nitish & Mudigere, Dheevatsa & Nocedal, Jorge & Smelyanskiy, Mikhail & Tang, Ping. (2016). On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. https://arxiv.org/abs/1609.04836


#### Dropout
1. Srivastava, Nitish & Hinton, Geoffrey & Krizhevsky, Alex & Sutskever, Ilya & Salakhutdinov, Ruslan. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research. 15. 1929-1958. https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf

#### Other 
1. Courbariaux, Matthieu & Hubara, Itay & Soudry, Daniel & El-Yaniv, Ran & Bengio, Y.. (2016). Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1. https://arxiv.org/pdf/1602.02830.pdf
2. Glorot, Xavier & Bengio, Y.. (2010). Understanding the difficulty of training deep feedforward neural networks. Journal of Machine Learning Research - Proceedings Track. 9. 249-256. https://proceedings.mlr.press/v9/glorot10a.html
3. Sarle, W. (1995). Stopped Training and Other Remedies for Overfitting. https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.3920


#### Perceptron
1. ROSENBLATT F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychol Rev. 1958 Nov;65(6):386-408. doi: 10.1037/h0042519. PMID: 13602029. https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf


## Recommender Systems
#### Collaborative Filtering
1. Goldberg, David et al. ‚ÄúUsing collaborative filtering to weave an information tapestry.‚Äù Commun. ACM 35 (1992): 61-70. https://dl.acm.org/doi/10.1145/138859.138867

#### Matrix Factorization
1. Sarwar, Badrul & Karypis, George & Konstan, Joseph & Riedl, John. (2000). Application of Dimensionality Reduction in Recommender System -- A Case Study. http://files.grouplens.org/papers/webKDD00.pdf
2. Daniel Billsus and Michael J. Pazzani. 1998. Learning Collaborative Information Filters. In Proceedings of the Fifteenth International Conference on Machine Learning (ICML '98). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 46‚Äì54. https://www.ics.uci.edu/~pazzani/Publications/MLC98.pdf

#### Implicit Matrix Factorization
1. Y. Hu, Y. Koren and C. Volinsky, "Collaborative Filtering for Implicit Feedback Datasets," 2008 Eighth IEEE International Conference on Data Mining, 2008, pp. 263-272, doi: 10.1109/ICDM.2008.22. http://yifanhu.net/PUB/cf.pdf

## Regression
#### Elastic Net
1. Zou, Hui & Hastie, Trevor. (2005). Regularization and variable selection via the elastic net (vol B 67, pg 301, 2005). Journal of the Royal Statistical Society Series B. 67. 768-768. 10.1111/j.1467-9868.2005.00527.x. https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2005.00503.x

#### Lasso
1. Tibshirani, Robert. ‚ÄúRegression Shrinkage and Selection via the Lasso.‚Äù Journal of the royal statistical society series b-methodological 58 (1996): 267-288. https://tibshirani.su.domains/ftp/lasso-retro.pdf

## Software
#### MapReduce
1. Dean, Jeffrey & Ghemawat, Sanjay. (2004). MapReduce: Simplified Data Processing on Large Clusters. Communications of the ACM. 51. 137-150. 10.1145/1327452.1327492. https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf

#### TensorFlow

1. Abadi, Mart√≠n & Barham, Paul & Chen, Jianmin & Chen, Zhifeng & Davis, Andy & Dean, Jeffrey & Devin, Matthieu & Ghemawat, Sanjay & Irving, Geoffrey & Isard, Michael & Kudlur, Manjunath & Levenberg, Josh & Monga, Rajat & Moore, Sherry & Murray, Derek & Steiner, Benoit & Tucker, Paul & Vasudevan, Vijay & Warden, Pete & Zhang, Xiaoqiang. (2016). TensorFlow: A system for large-scale machine learning. https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf

#### Torch
1. Collobert, Ronan & Bengio, Samy & Marithoz, Johnny. (2002). Torch: A Modular Machine Learning Software Library. https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.8.9850

## Supervised Learning
#### k-Nearest Neighbors
1. Cover, Thomas M. and Peter E. Hart. ‚ÄúNearest neighbor pattern classification.‚Äù IEEE Trans. Inf. Theory 13 (1967): 21-27. https://ieeexplore.ieee.org/document/1053964

#### Support Vector Machine
1. Cortes, C., Vapnik, V. Support-vector networks. Mach Learn 20, 273‚Äì297 (1995). https://doi.org/10.1007/BF00994018. https://link.springer.com/article/10.1007/BF00994018

## Statistics
#### The Bootstrap
1. Efron, B. (1992). Bootstrap Methods: Another Look at the Jackknife. In: Kotz, S., Johnson, N.L. (eds) Breakthroughs in Statistics. Springer Series in Statistics. Springer, New York, NY. https://doi.org/10.1007/978-1-4612-4380-9_41. https://sites.stat.washington.edu/courses/stat527/s14/readings/ann_stat1979.pdf


# Applications
## Biology
- Papers on machine learning for proteins https://github.com/yangkky/Machine-learning-for-proteins
- Awesome-machine-or-deep-learning-related-papers-on-Cell-Nature-Science-series-of-journals https://github.com/jdlc105/Awesome-machine-or-deep-learning-related-papers-on-Cell-Nature-Science-series-of-journals

## Biomedicine
An-Incomplete-ML-Paper-Collection-for-BioMedical-Applications https://github.com/xuhanvsxuhan/An-Incomplete-ML-Paper-Collection-for-BioMedical-Applications

## Business 
- Data science & machine learning in production https://github.com/eugeneyan/applied-ml

## Machine Learning Engineering
- awesome-machine-learning-engineering https://github.com/d18s/awesome-machine-learning-engineering

## Medicine
- Deep Learning papers in Electronic Health Records (EHR) https://github.com/gdorleon/DeepLearning_EHR_Papers

##  Physics 
- NeurIPS 2019 Papers/Talks Relevant to Computational Physics https://github.com/gdportwood/Turb_ML_Papers/blob/master/NeurIPS2019.md
- Computational-Physics-and-Machine-Learning-Reading-List https://github.com/loliverhennigh/Computational-Physics-and-Machine-Learning-Reading-List
- A Living Review of Machine Learning for Particle Physics https://github.com/iml-wg/HEPML-LivingReview

## Software Engineering
- Machine Learning for Software Engineering https://github.com/saltudelft/ml4se


